{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习\n",
    "\n",
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "1. averaging methods（取平均方法） \n",
    "\n",
    "the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "\n",
    "该方法的思想是训练多个估值器，取预测值的均值。\n",
    "    \n",
    "    Examples: Bagging methods, Forests of randomized trees, ...\n",
    "\n",
    "2. boosting methods(提升方法)\n",
    "\n",
    "base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "该方法是将多个弱的方法合成强的方法。\n",
    "\n",
    "    Examples: AdaBoost, Gradient Tree Boosting, ...\n",
    "    \n",
    "## Bagging\n",
    "\n",
    "In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees).\n",
    "\n",
    " bagging方法是从大小为n的原始训练数据集D中随机选择$n′(n′<n)$个样本点组成一个新的训练集，这个选择过程独立重复B次。然后，每一个新的训练集都被独立的用于训练一个子分类器，最终分类器的分类结果由这些子分类器投票决定。\n",
    "\n",
    "\n",
    "## Bootstrap\n",
    "\n",
    "In scikit-learn, bagging methods are offered as a unified BaggingClassifier meta-estimator (resp. BaggingRegressor), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, max_samples and max_features control the size of the subsets (in terms of samples and features), while bootstrap and bootstrap_features control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization error can be estimated with the out-of-bag samples by setting oob_score=True. As an example, the snippet below illustrates how to instantiate a bagging ensemble of KNeighborsClassifier base estimators, each built on random subsets of 50% of the samples and 50% of the features.\n",
    "\n",
    "Bootstrap是估计统计量的重采样方法,它是从大小为n的原始训练数据集D中随机选择n个样本点组成一个新的训练集，这个选择过程独立重复B次，然后用这B个数据集对模型统计量进行估计（如均值、方差等）。由于原始数据集的大小就是n，所以这B个新的训练集中不可避免的会存在重复的样本。\n",
    "\n",
    "\n",
    "[区分bootstrap、bagging、boosting和adaboost](http://blog.csdn.net/wangjian1204/article/details/50668929)\n",
    "\n",
    "\n",
    "## Boosting\n",
    "\n",
    "\n",
    "boosting依次训练k个子分类器，最终的分类结果由这些子分类器投票决定。首先从大小为n的原始训练数据集中随机选取$n_1$个样本训练出第一个分类器，记为$C_1$，然后构造第二个分类器$C_2$的训练集$D_2$，要求：$D_2$中一半样本能被$C_1$正确分类，而另一半样本被$C_1$错分。接着继续构造第三个分类器$C_3$的训练集$D_3$，要求：$C_1$、$C_2$对$D_3$中样本的分类结果不同。剩余的子分类器按照类似的思路进行训练。  boosting构造新训练集的主要原则是使用最富信息的样本。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
